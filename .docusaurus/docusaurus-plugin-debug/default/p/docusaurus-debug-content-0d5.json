{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"E:\\Quarter-4\\Q4\\Hackathons\\robotics-book\\sidebars.js","contentPath":"E:\\Quarter-4\\Q4\\Hackathons\\robotics-book\\docs","docs":[{"id":"chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","title":"Lesson 1.1 — What is Physical AI","description":"Physical AI refers to artificial intelligence systems that interact with the real world through physical bodies, sensors, and actuators. Unlike traditional AI, which exists only as software, Physical AI combines cognition, perception, and action in a single system.","source":"@site/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai.md","sourceDirName":"chapter-01-introduction-to-physical-ai","slug":"/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai.md","tags":[],"version":"current","frontMatter":{"id":"lesson-1-1-what-is-physical-ai","title":"Lesson 1.1 — What is Physical AI","sidebar_label":"Lesson 1.1: What is Physical AI","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","next":{"title":"Lesson 1.2: Embodied Intelligence","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied intelligence"}},{"id":"chapter-01-introduction-to-physical-ai/lesson-1-2-embodied intelligence","title":"Lesson 1.2 — Embodied Intelligence","description":"Embodied Intelligence is the principle that intelligence emerges through interaction between a body and its environment. Physical AI leverages this by combining sensors, actuators, and AI algorithms in a feedback loop.","source":"@site/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence.md","sourceDirName":"chapter-01-introduction-to-physical-ai","slug":"/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied intelligence","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied intelligence","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence.md","tags":[],"version":"current","frontMatter":{"id":"lesson-1-2-embodied intelligence","title":"Lesson 1.2 — Embodied Intelligence","sidebar_label":"Lesson 1.2: Embodied Intelligence","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.1: What is Physical AI","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai"},"next":{"title":"Lesson 1.3: Course Overview & Learning Outcomes","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course overview & learning outcomes"}},{"id":"chapter-01-introduction-to-physical-ai/lesson-1-3-course overview & learning outcomes","title":"Lesson 1.3 — Course Overview & Learning Outcomes","description":"This course introduces students to Physical AI systems and covers:","source":"@site/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview.md","sourceDirName":"chapter-01-introduction-to-physical-ai","slug":"/chapter-01-introduction-to-physical-ai/lesson-1-3-course overview & learning outcomes","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course overview & learning outcomes","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview.md","tags":[],"version":"current","frontMatter":{"id":"lesson-1-3-course overview & learning outcomes","title":"Lesson 1.3 — Course Overview & Learning Outcomes","sidebar_label":"Lesson 1.3: Course Overview & Learning Outcomes","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.2: Embodied Intelligence","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied intelligence"},"next":{"title":"Lesson 2.1: ROS 2 Architecture","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-rOS 2 architecture"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-1-rOS 2 architecture","title":"Lesson 2.1 — ROS 2 Architecture","description":"ROS 2 (Robot Operating System 2) is a framework for developing robot software. It provides modular architecture, communication infrastructure, and tools for building complex robotic systems.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-rOS 2 architecture","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-rOS 2 architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture.md","tags":[],"version":"current","frontMatter":{"id":"lesson-2-1-rOS 2 architecture","title":"Lesson 2.1 — ROS 2 Architecture","sidebar_label":"Lesson 2.1: ROS 2 Architecture","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.3: Course Overview & Learning Outcomes","permalink":"/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course overview & learning outcomes"},"next":{"title":"Lesson 2.2: Nodes, Topics, Services, Actions","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","title":"Lesson 2.2 — Nodes, Topics, Services, and Actions","description":"ROS 2 nodes interact using topics, services, and actions. This enables modular, decoupled robot software that can be scaled.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions.md","tags":[],"version":"current","frontMatter":{"id":"lesson-2-2-nodes-topics-services-actions","title":"Lesson 2.2 — Nodes, Topics, Services, and Actions","sidebar_label":"Lesson 2.2: Nodes, Topics, Services, Actions","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.1: ROS 2 Architecture","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-rOS 2 architecture"},"next":{"title":"Lesson 2.3: Python Integration","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python integration"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python integration","title":"Lesson 2.3 — Python Integration","description":"Python is commonly used to write ROS 2 nodes due to its simplicity and rich libraries.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python integration","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python integration","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration.md","tags":[],"version":"current","frontMatter":{"id":"lesson-2-3-python integration","title":"Lesson 2.3 — Python Integration","sidebar_label":"Lesson 2.3: Python Integration","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.2: Nodes, Topics, Services, Actions","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions"},"next":{"title":"Lesson 2.4: URDF for Humanoids","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf for humanoids"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf for humanoids","title":"Lesson 2.4 — URDF for Humanoids","description":"URDF (Unified Robot Description Format) is an XML-based format used to describe the structure and properties of robots in ROS 2. It allows simulation, visualization, and integration of humanoid robots in Gazebo or RViz.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf for humanoids","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf for humanoids","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids.md","tags":[],"version":"current","frontMatter":{"id":"lesson-2-4-urdf for humanoids","title":"Lesson 2.4 — URDF for Humanoids","sidebar_label":"Lesson 2.4: URDF for Humanoids","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.3: Python Integration","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python integration"},"next":{"title":"Lesson 3.1: Gazebo Simulation Basics","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo simulation basics"}},{"id":"chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo simulation basics","title":"Lesson 3.1 — LGazebo Simulation Basics","description":"Gazebo is a powerful 3D robotics simulator that allows testing and development of robots in virtual environments before deploying to the real world.","source":"@site/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics.md","sourceDirName":"chapter-03-digital-twin-gazebo-unity","slug":"/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo simulation basics","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo simulation basics","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics.md","tags":[],"version":"current","frontMatter":{"id":"lesson-3-1-gazebo simulation basics","title":"Lesson 3.1 — LGazebo Simulation Basics","sidebar_label":"Lesson 3.1: Gazebo Simulation Basics","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.4: URDF for Humanoids","permalink":"/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf for humanoids"},"next":{"title":"Lesson 3.2: Sensor Simulation","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor simulation"}},{"id":"chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor simulation","title":"Lesson 3.2 — Sensor Simulation","description":"Simulating sensors in Gazebo or Unity allows testing perception and control algorithms without a real robot.","source":"@site/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation.md","sourceDirName":"chapter-03-digital-twin-gazebo-unity","slug":"/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor simulation","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation.md","tags":[],"version":"current","frontMatter":{"id":"lesson-3-2-sensor simulation","title":"Lesson 3.2 — Sensor Simulation","sidebar_label":"Lesson 3.2: Sensor Simulation","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.1: Gazebo Simulation Basics","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo simulation basics"},"next":{"title":"Lesson 3.3: High-Fidelity Rendering with Unity","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity rendering with unity"}},{"id":"chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity rendering with unity","title":"Lesson 3.3 — High-Fidelity Rendering with Unity","description":"High-fidelity rendering ensures robots and environments look realistic, which helps in visualization, debugging, and AI perception development.","source":"@site/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity.md","sourceDirName":"chapter-03-digital-twin-gazebo-unity","slug":"/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity rendering with unity","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity rendering with unity","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity.md","tags":[],"version":"current","frontMatter":{"id":"lesson-3-3-high-fidelity rendering with unity","title":"Lesson 3.3 — High-Fidelity Rendering with Unity","sidebar_label":"Lesson 3.3: High-Fidelity Rendering with Unity","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.2: Sensor Simulation","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor simulation"},"next":{"title":"Lesson 4.1: Isaac Sim Overview","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview"}},{"id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","title":"Lesson 4.1 — Isaac Sim Overview","description":"NVIDIA Isaac Sim is a robotics simulation platform that enables testing AI algorithms in photorealistic 3D environments.","source":"@site/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview.md","sourceDirName":"chapter-04-ai-robot-brain-nvidia-isaac","slug":"/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview.md","tags":[],"version":"current","frontMatter":{"id":"lesson-4-1-isaac-sim-overview","title":"Lesson 4.1 — Isaac Sim Overview","sidebar_label":"Lesson 4.1: Isaac Sim Overview","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.3: High-Fidelity Rendering with Unity","permalink":"/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity rendering with unity"},"next":{"title":"Lesson 4.2: Isaac ROS for Navigation","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-navigation"}},{"id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-navigation","title":"Lesson 4.2 — Isaac ROS for Navigation","description":"Isaac ROS provides prebuilt ROS 2 packages and nodes for robot navigation, including path planning, mapping, and control.","source":"@site/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation.md","sourceDirName":"chapter-04-ai-robot-brain-nvidia-isaac","slug":"/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-navigation","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation.md","tags":[],"version":"current","frontMatter":{"id":"lesson-4-2-isaac-ros-navigation","title":"Lesson 4.2 — Isaac ROS for Navigation","sidebar_label":"Lesson 4.2: Isaac ROS for Navigation","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1: Isaac Sim Overview","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview"},"next":{"title":"Lesson 4.3: RL & Sim-to-Real","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-rl-sim-to-real"}},{"id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-rl-sim-to-real","title":"Lesson 4.3 — Reinforcement Learning & Sim-to-Real Transfer","description":"Reinforcement Learning (RL) allows robots to learn tasks by trial and error in simulation before transferring skills to real robots.","source":"@site/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning.md","sourceDirName":"chapter-04-ai-robot-brain-nvidia-isaac","slug":"/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-rl-sim-to-real","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-rl-sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning.md","tags":[],"version":"current","frontMatter":{"id":"lesson-4-3-rl-sim-to-real","title":"Lesson 4.3 — Reinforcement Learning & Sim-to-Real Transfer","sidebar_label":"Lesson 4.3: RL & Sim-to-Real","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.2: Isaac ROS for Navigation","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-navigation"},"next":{"title":"Lesson 5.1: LLM Integration","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration"}},{"id":"chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","title":"Lesson 5.1 — LLM Integration","description":"Large Language Models (LLMs) can be integrated into robots to enable understanding of natural language commands and reasoning.","source":"@site/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration.md","sourceDirName":"chapter-05-vision-language-action-vla","slug":"/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration.md","tags":[],"version":"current","frontMatter":{"id":"lesson-5-1-llm-integration","title":"Lesson 5.1 — LLM Integration","sidebar_label":"Lesson 5.1: LLM Integration","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.3: RL & Sim-to-Real","permalink":"/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-rl-sim-to-real"},"next":{"title":"Lesson 5.2: Voice-to-Action","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action"}},{"id":"chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action","title":"Lesson 5.2 — Voice-to-Action with Whisper","description":"Voice-to-Action allows robots to understand spoken commands using speech recognition models like Whisper.","source":"@site/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper.md","sourceDirName":"chapter-05-vision-language-action-vla","slug":"/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper.md","tags":[],"version":"current","frontMatter":{"id":"lesson-5-2-voice-to-action","title":"Lesson 5.2 — Voice-to-Action with Whisper","sidebar_label":"Lesson 5.2: Voice-to-Action","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5.1: LLM Integration","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration"},"next":{"title":"Lesson 5.3: Cognitive Planning","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning"}},{"id":"chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","title":"Lesson 5.3 — Cognitive Planning & Multi-modal Interaction","description":"Cognitive planning allows robots to combine multiple sensory inputs and reasoning for complex task execution.","source":"@site/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning.md","sourceDirName":"chapter-05-vision-language-action-vla","slug":"/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning.md","tags":[],"version":"current","frontMatter":{"id":"lesson-5-3-cognitive-planning","title":"Lesson 5.3 — Cognitive Planning & Multi-modal Interaction","sidebar_label":"Lesson 5.3: Cognitive Planning","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5.2: Voice-to-Action","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action"},"next":{"title":"Lesson 6.1: Autonomous Humanoid Overview","permalink":"/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview"}},{"id":"chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","title":"Lesson 6.1 — Autonomous Humanoid Overview","description":"The Capstone Project focuses on building an autonomous humanoid robot by integrating concepts from all previous chapters.","source":"@site/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview.md","sourceDirName":"chapter-06-capstone-project","slug":"/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","permalink":"/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview.md","tags":[],"version":"current","frontMatter":{"id":"lesson-6-1-autonomous-humanoid-overview","title":"Lesson 6.1 — Autonomous Humanoid Overview","sidebar_label":"Lesson 6.1: Autonomous Humanoid Overview","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5.3: Cognitive Planning","permalink":"/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning"},"next":{"title":"Lesson 6.2: Integration of Modules","permalink":"/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules"}},{"id":"chapter-06-capstone-project/lesson-6-2-integration-of-modules","title":"Lesson 6.2 — Integration of Modules","description":"Integration combines perception, planning, control, and AI reasoning into a cohesive humanoid system.","source":"@site/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules.md","sourceDirName":"chapter-06-capstone-project","slug":"/chapter-06-capstone-project/lesson-6-2-integration-of-modules","permalink":"/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules.md","tags":[],"version":"current","frontMatter":{"id":"lesson-6-2-integration-of-modules","title":"Lesson 6.2 — Integration of Modules","sidebar_label":"Lesson 6.2: Integration of Modules","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.1: Autonomous Humanoid Overview","permalink":"/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview"},"next":{"title":"Lesson 6.3: Deployment & Assessment","permalink":"/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment"}},{"id":"chapter-06-capstone-project/lesson-6-3-deployment-assessment","title":"Lesson 6.3 — Deployment & Assessment","description":"Deployment involves testing the humanoid in real-world scenarios and evaluating performance.","source":"@site/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment.md","sourceDirName":"chapter-06-capstone-project","slug":"/chapter-06-capstone-project/lesson-6-3-deployment-assessment","permalink":"/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment.md","tags":[],"version":"current","frontMatter":{"id":"lesson-6-3-deployment-assessment","title":"Lesson 6.3 — Deployment & Assessment","sidebar_label":"Lesson 6.3: Deployment & Assessment","sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.2: Integration of Modules","permalink":"/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules"},"next":{"title":"Introduction","permalink":"/docs/intro"}},{"id":"intro","title":"Introduction","description":"This book is designed to guide you through the exciting world of robotics, from basic concepts to advanced simulations and humanoid robot control. Throughout the chapters, you'll explore:","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/alishasami645/Physical-AI-Humanoid-Robotics/tree/main/docs/intro.md","tags":[],"version":"current","frontMatter":{"id":"intro","title":"Introduction","hide_title":false},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.3: Deployment & Assessment","permalink":"/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"category","label":"Chapter 1: Introduction to Physical AI","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","label":"Lesson 1.1: What is Physical AI"},{"type":"doc","id":"chapter-01-introduction-to-physical-ai/lesson-1-2-embodied intelligence","label":"Lesson 1.2: Embodied Intelligence"},{"type":"doc","id":"chapter-01-introduction-to-physical-ai/lesson-1-3-course overview & learning outcomes","label":"Lesson 1.3: Course Overview & Learning Outcomes"}]},{"type":"category","label":"Chapter 2: Robotic Nervous System (ROS 2)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-1-rOS 2 architecture","label":"Lesson 2.1: ROS 2 Architecture"},{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","label":"Lesson 2.2: Nodes, Topics, Services, Actions"},{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python integration","label":"Lesson 2.3: Python Integration"},{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf for humanoids","label":"Lesson 2.4: URDF for Humanoids"}]},{"type":"category","label":"Chapter 3: Digital Twin (Gazebo & Unity)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo simulation basics","label":"Lesson 3.1: Gazebo Simulation Basics"},{"type":"doc","id":"chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor simulation","label":"Lesson 3.2: Sensor Simulation"},{"type":"doc","id":"chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity rendering with unity","label":"Lesson 3.3: High-Fidelity Rendering with Unity"}]},{"type":"category","label":"Chapter 4: AI-Robot Brain (NVIDIA Isaac)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","label":"Lesson 4.1: Isaac Sim Overview"},{"type":"doc","id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-navigation","label":"Lesson 4.2: Isaac ROS for Navigation"},{"type":"doc","id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-rl-sim-to-real","label":"Lesson 4.3: RL & Sim-to-Real"}]},{"type":"category","label":"Chapter 5: Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","label":"Lesson 5.1: LLM Integration"},{"type":"doc","id":"chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action","label":"Lesson 5.2: Voice-to-Action"},{"type":"doc","id":"chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","label":"Lesson 5.3: Cognitive Planning"}]},{"type":"category","label":"Chapter 6: Capstone Project","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","label":"Lesson 6.1: Autonomous Humanoid Overview"},{"type":"doc","id":"chapter-06-capstone-project/lesson-6-2-integration-of-modules","label":"Lesson 6.2: Integration of Modules"},{"type":"doc","id":"chapter-06-capstone-project/lesson-6-3-deployment-assessment","label":"Lesson 6.3: Deployment & Assessment"}]},{"type":"doc","id":"intro"}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false},{"type":"jsx","permalink":"/signin","source":"@site/src/pages/signin.tsx"},{"type":"jsx","permalink":"/signup","source":"@site/src/pages/signup.tsx"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}