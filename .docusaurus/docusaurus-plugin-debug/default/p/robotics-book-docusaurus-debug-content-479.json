{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/robotics-book/docs","tagsPath":"/robotics-book/docs/tags","editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"E:\\Quarter-4\\Q4\\Hackathons\\robotics-book\\sidebars.js","contentPath":"E:\\Quarter-4\\Q4\\Hackathons\\robotics-book\\docs","docs":[{"id":"chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","title":"Lesson 1.1: What is Physical AI","description":"Physical AI refers to artificial intelligence systems that interact with the real world through physical bodies, sensors, and actuators. Unlike traditional AI, which exists only as software, Physical AI combines cognition, perception, and action in a single system.","source":"@site/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai.md","sourceDirName":"chapter-01-introduction-to-physical-ai","slug":"/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","next":{"title":"Lesson 1.2: Embodied Intelligence","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence"}},{"id":"chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence","title":"Lesson 1.2: Embodied Intelligence","description":"Embodied Intelligence is the principle that intelligence emerges through interaction between a body and its environment. Physical AI leverages this by combining sensors, actuators, and AI algorithms in a feedback loop.","source":"@site/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence.md","sourceDirName":"chapter-01-introduction-to-physical-ai","slug":"/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.1: What is Physical AI","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai"},"next":{"title":"Lesson 1.3: Course Overview & Learning Outcomes","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview"}},{"id":"chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview","title":"Lesson 1.3: Course Overview & Learning Outcomes","description":"This course introduces students to Physical AI systems and covers:","source":"@site/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview.md","sourceDirName":"chapter-01-introduction-to-physical-ai","slug":"/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.2: Embodied Intelligence","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence"},"next":{"title":"Lesson 2.1: ROS 2 Architecture","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture","title":"Lesson 2.1: ROS 2 Architecture","description":"ROS 2 (Robot Operating System 2) is a framework for developing robot software. It provides modular architecture, communication infrastructure, and tools for building complex robotic systems.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.3: Course Overview & Learning Outcomes","permalink":"/robotics-book/docs/chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview"},"next":{"title":"Lesson 2.2: Nodes, Topics, Services, Actions","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","title":"Lesson 2.2: Nodes, Topics, Services, Actions","description":"ROS 2 nodes interact using topics, services, and actions. This enables modular, decoupled robot software that can be scaled.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.1: ROS 2 Architecture","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture"},"next":{"title":"Lesson 2.3: Python Integration","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration","title":"Lesson 2.3: Python Integration","description":"Python is commonly used to write ROS 2 nodes due to its simplicity and rich libraries.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.2: Nodes, Topics, Services, Actions","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions"},"next":{"title":"Lesson 2.4: URDF for Humanoids","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids"}},{"id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids","title":"Lesson 2.4: URDF for Humanoids","description":"URDF (Unified Robot Description Format) is an XML-based format used to describe the structure and properties of robots in ROS 2. It allows simulation, visualization, and integration of humanoid robots in Gazebo or RViz.","source":"@site/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids.md","sourceDirName":"chapter-02-robotic-nervous-system-ros-2","slug":"/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.3: Python Integration","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration"},"next":{"title":"Lesson 3.1: Gazebo Simulation Basics","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics"}},{"id":"chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics","title":"Lesson 3.1: Gazebo Simulation Basics","description":"Gazebo is a powerful 3D robotics simulator that allows testing and development of robots in virtual environments before deploying to the real world.","source":"@site/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics.md","sourceDirName":"chapter-03-digital-twin-gazebo-unity","slug":"/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.4: URDF for Humanoids","permalink":"/robotics-book/docs/chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids"},"next":{"title":"Lesson 3.2: Sensor Simulation (LiDAR, Cameras, IMU)","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation"}},{"id":"chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation","title":"Lesson 3.2: Sensor Simulation (LiDAR, Cameras, IMU)","description":"Simulating sensors in Gazebo or Unity allows testing perception and control algorithms without a real robot.","source":"@site/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation.md","sourceDirName":"chapter-03-digital-twin-gazebo-unity","slug":"/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.1: Gazebo Simulation Basics","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics"},"next":{"title":"Lesson 3.3: High-Fidelity Rendering with Unity","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity"}},{"id":"chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity","title":"Lesson 3.3: High-Fidelity Rendering with Unity","description":"High-fidelity rendering ensures robots and environments look realistic, which helps in visualization, debugging, and AI perception development.","source":"@site/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity.md","sourceDirName":"chapter-03-digital-twin-gazebo-unity","slug":"/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.2: Sensor Simulation (LiDAR, Cameras, IMU)","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation"},"next":{"title":"Lesson 4.1: Isaac Sim Overview","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview"}},{"id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","title":"Lesson 4.1: Isaac Sim Overview","description":"NVIDIA Isaac Sim is a robotics simulation platform that enables testing AI algorithms in photorealistic 3D environments.","source":"@site/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview.md","sourceDirName":"chapter-04-ai-robot-brain-nvidia-isaac","slug":"/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.3: High-Fidelity Rendering with Unity","permalink":"/robotics-book/docs/chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity"},"next":{"title":"Lesson 4.2: Isaac ROS for Navigation","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation"}},{"id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation","title":"Lesson 4.2: Isaac ROS for Navigation","description":"Isaac ROS provides prebuilt ROS 2 packages and nodes for robot navigation, including path planning, mapping, and control.","source":"@site/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation.md","sourceDirName":"chapter-04-ai-robot-brain-nvidia-isaac","slug":"/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1: Isaac Sim Overview","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview"},"next":{"title":"Lesson 4.3: Reinforcement Learning & Sim-to-Real Transfer","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning"}},{"id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning","title":"Lesson 4.3: Reinforcement Learning & Sim-to-Real Transfer","description":"Reinforcement Learning (RL) allows robots to learn tasks by trial and error in simulation before transferring skills to real robots.","source":"@site/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning.md","sourceDirName":"chapter-04-ai-robot-brain-nvidia-isaac","slug":"/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.2: Isaac ROS for Navigation","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation"},"next":{"title":"Lesson 5.1: LLM Integration","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration"}},{"id":"chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","title":"Lesson 5.1: LLM Integration","description":"Large Language Models (LLMs) can be integrated into robots to enable understanding of natural language commands and reasoning.","source":"@site/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration.md","sourceDirName":"chapter-05-vision-language-action-vla","slug":"/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.3: Reinforcement Learning & Sim-to-Real Transfer","permalink":"/robotics-book/docs/chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning"},"next":{"title":"Lesson 5.2: Voice-to-Action with Whisper","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper"}},{"id":"chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper","title":"Lesson 5.2: Voice-to-Action with Whisper","description":"Voice-to-Action allows robots to understand spoken commands using speech recognition models like Whisper.","source":"@site/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper.md","sourceDirName":"chapter-05-vision-language-action-vla","slug":"/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5.1: LLM Integration","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-1-llm-integration"},"next":{"title":"Lesson 5.3: Cognitive Planning & Multi-modal Interaction","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning"}},{"id":"chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","title":"Lesson 5.3: Cognitive Planning & Multi-modal Interaction","description":"Cognitive planning allows robots to combine multiple sensory inputs and reasoning for complex task execution.","source":"@site/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning.md","sourceDirName":"chapter-05-vision-language-action-vla","slug":"/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5.2: Voice-to-Action with Whisper","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper"},"next":{"title":"Lesson 6.1: Autonomous Humanoid Overview","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview"}},{"id":"chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","title":"Lesson 6.1: Autonomous Humanoid Overview","description":"The Capstone Project focuses on building an autonomous humanoid robot by integrating concepts from all previous chapters.","source":"@site/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview.md","sourceDirName":"chapter-06-capstone-project","slug":"/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5.3: Cognitive Planning & Multi-modal Interaction","permalink":"/robotics-book/docs/chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning"},"next":{"title":"Lesson 6.2: Integration of Modules","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules"}},{"id":"chapter-06-capstone-project/lesson-6-2-integration-of-modules","title":"Lesson 6.2: Integration of Modules","description":"Integration combines perception, planning, control, and AI reasoning into a cohesive humanoid system.","source":"@site/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules.md","sourceDirName":"chapter-06-capstone-project","slug":"/chapter-06-capstone-project/lesson-6-2-integration-of-modules","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.1: Autonomous Humanoid Overview","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview"},"next":{"title":"Lesson 6.3: Deployment & Assessment","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment"}},{"id":"chapter-06-capstone-project/lesson-6-3-deployment-assessment","title":"Lesson 6.3: Deployment & Assessment","description":"Deployment involves testing the humanoid in real-world scenarios and evaluating performance.","source":"@site/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment.md","sourceDirName":"chapter-06-capstone-project","slug":"/chapter-06-capstone-project/lesson-6-3-deployment-assessment","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.2: Integration of Modules","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-2-integration-of-modules"},"next":{"title":"Introduction","permalink":"/robotics-book/docs/intro"}},{"id":"intro","title":"Introduction","description":"This book is designed to guide you through the exciting world of robotics, from basic concepts to advanced simulations and humanoid robot control. Throughout the chapters, you'll explore:","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/robotics-book/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/YOUR_USERNAME/robotics-book/tree/main/docs/intro.md","tags":[],"version":"current","frontMatter":{"id":"intro","title":"Introduction","hide_title":false},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.3: Deployment & Assessment","permalink":"/robotics-book/docs/chapter-06-capstone-project/lesson-6-3-deployment-assessment"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"category","label":"Chapter 1: Introduction to Physical AI","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-01-introduction-to-physical-ai/lesson-1-1-what-is-physical-ai"},{"type":"doc","id":"chapter-01-introduction-to-physical-ai/lesson-1-2-embodied-intelligence"},{"type":"doc","id":"chapter-01-introduction-to-physical-ai/lesson-1-3-course-overview"}]},{"type":"category","label":"Chapter 2: Robotic Nervous System (ROS 2)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-1-ros-2-architecture"},{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-2-nodes-topics-services-actions"},{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-3-python-integration"},{"type":"doc","id":"chapter-02-robotic-nervous-system-ros-2/lesson-2-4-urdf-for-humanoids"}]},{"type":"category","label":"Chapter 3: Digital Twin (Gazebo & Unity)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-03-digital-twin-gazebo-unity/lesson-3-1-gazebo-simulation-basics"},{"type":"doc","id":"chapter-03-digital-twin-gazebo-unity/lesson-3-2-sensor-simulation"},{"type":"doc","id":"chapter-03-digital-twin-gazebo-unity/lesson-3-3-high-fidelity-rendering-with-unity"}]},{"type":"category","label":"Chapter 4: AI-Robot Brain (NVIDIA Isaac)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-1-isaac-sim-overview"},{"type":"doc","id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-2-isaac-ros-for-navigation"},{"type":"doc","id":"chapter-04-ai-robot-brain-nvidia-isaac/lesson-4-3-reinforcement-learning"}]},{"type":"category","label":"Chapter 5: Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-05-vision-language-action-vla/lesson-5-1-llm-integration"},{"type":"doc","id":"chapter-05-vision-language-action-vla/lesson-5-2-voice-to-action-with-whisper"},{"type":"doc","id":"chapter-05-vision-language-action-vla/lesson-5-3-cognitive-planning"}]},{"type":"category","label":"Chapter 6: Capstone Project","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"chapter-06-capstone-project/lesson-6-1-autonomous-humanoid-overview"},{"type":"doc","id":"chapter-06-capstone-project/lesson-6-2-integration-of-modules"},{"type":"doc","id":"chapter-06-capstone-project/lesson-6-3-deployment-assessment"}]},{"type":"doc","id":"intro"}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/robotics-book/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/robotics-book/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/robotics-book/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}